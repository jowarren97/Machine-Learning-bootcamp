REINFORCEMENT LEARNING

3 types:
Supervised
Unsupervised
Reinforcement learning (RL) - good for evolving dataset i.e. interaction w environment

Deep RL = Q networks + deep neural networks

Reward - single scalar value e.g. +1 for chess win, -1 for loss
Want to maximise cummulative reward
Reward must be defined by environment or researcher
What are the correct rewards? Objectives vs values

Want to maximise future reward (not just short term immediate reward) i.e. delayed gratification

Coupled system betw agent and environment
Action -> Observation, Reward

History is memory of observations, rewards, actions
State enables decision making
State is a function of history

Markov
S(t+1) depends only on S(t), indep of past states
i.e. all history contained in current state 
Memory stored in weights of neural network (like Neurodynamics, attractor environments)

Discount factor - necessary to consider how far in future we want to look (rewards might make an infinite sum if we consider all)

-----------

RL agent may/may not have 3 features:

Policy - behaviour
Function maps state to action
Can be deterministic i.e. if state i do action j
Can be stochastic i.e. proby of action j given state i
Stationary as time independent, depends only on current state
Number of actions can be either finite (e.g. up, down, left, right in atari game) or infinite (e.g. robot arm position)

Value function - prediction of future reward
Use to select action

Model - predicts not just of reward but also next state

-----------

RL Classifications:

Policy-based
Value-based*
Policy & Value-based (Actor-Critic)

Model-based
Model-free

-----------

EXPLORATION (find info abt env) vs EXPLOTATION (of known info to maximise reward)
Dont want to stay in sub-optimal policy

-----------

Markov Decision Processes (Markov chain w arrows betw states corresponding to actions)
Current state --- Action ---> a) Reward b) New state that we can observe

Partially Observable Markov Decision Processes (e.g. poker - observations hidden)

-----------

Q LEARNING

V numbers, state-value functions: Expected value of rewards if we start in state S. Represents how good a state is
Q numbers, action-state-value functions: Goes step beyond. Calc expected reward depending on which action we take from state S.     Represents how good an action is if in a particular state

e.g. Atari game:
Q(s,up)
Q(s,right)
Q(s,left)
etc.

Choose one which has highest expected reward.
Use as policy to act

Optimal player would have optimal Q and V numbers Q*, V*
In Q learning, we learn the Q* values
Impractical to compute Q*
Need to generalise, account for fact that many states are similar & have similar potential reward
Use 'Experience Replay' - interaction (i.e. rewards + observations) to iteratively improve estimations of Q*

Bellman Eqn! Use 100-e% of time, e% ignore and do something random just to explore all possible states
i.e. 100-e % take greedy action, e % random - balance EXPLOITATION vs EXPLORATION

-----------

NEURAL NETWORKS

Use convolutional neural networks - good for computer vision (i.e. pixels in Atari). CS231n Stanford
Recognise relevant features from background
Input: state, Output: state-action-value function












